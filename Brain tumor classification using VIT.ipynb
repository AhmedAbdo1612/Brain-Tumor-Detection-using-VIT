{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZyK6ifS7mlT",
        "outputId": "04023f92-623f-4530-9c9d-1148641990ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Y_BTwOJn76Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir dataset"
      ],
      "metadata": {
        "id": "qm46MPgw77I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d denizkavi1/brain-tumor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gGHu-IL8Jnc",
        "outputId": "e2e86c88-d7a3-4ce5-bee2-5c88b0857b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading brain-tumor.zip to /content\n",
            "100% 699M/700M [00:35<00:00, 21.4MB/s]\n",
            "100% 700M/700M [00:35<00:00, 20.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/brain-tumor.zip -d dataset"
      ],
      "metadata": {
        "id": "cvsqDspx8Dyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hugsvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "delcZHI79JBT",
        "outputId": "a939441f-3a19-4cfd-d179-d0c6aaa75b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hugsvision\n",
            "  Downloading hugsvision-0.75.5-py3-none-any.whl (26 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from hugsvision) (3.2.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from hugsvision) (1.0.2)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from hugsvision) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from hugsvision) (4.64.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from hugsvision) (4.6.0.66)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from hugsvision) (1.13.1+cu116)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from hugsvision) (0.8.10)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from hugsvision) (0.14.1+cu116)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from hugsvision) (2.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->hugsvision) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->hugsvision) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->hugsvision) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib->hugsvision) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->hugsvision) (3.0.9)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->hugsvision) (6.0)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->hugsvision) (4.4.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->hugsvision) (2022.11.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->hugsvision) (21.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->hugsvision) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->hugsvision) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->hugsvision) (1.2.0)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->hugsvision) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->hugsvision) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->hugsvision) (3.9.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (3.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->hugsvision) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->hugsvision) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->hugsvision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->hugsvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->hugsvision) (2022.12.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->hugsvision) (1.3.3)\n",
            "Installing collected packages: tokenizers, torchmetrics, lightning-utilities, huggingface-hub, transformers, timm, pytorch-lightning, hugsvision\n",
            "Successfully installed huggingface-hub-0.12.0 hugsvision-0.75.5 lightning-utilities-0.6.0.post0 pytorch-lightning-1.9.0 timm-0.6.12 tokenizers-0.13.2 torchmetrics-0.11.1 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/dataset'\n",
        "%ls -la\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9kQe_sH_TVS",
        "outputId": "75d0dd4f-3a6e-459c-9474-d60bc6c976f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n",
            "total 88\n",
            "drwxr-xr-x 5 root root  4096 Feb  1 22:06 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root  4096 Feb  1 22:05 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Feb  1 22:06 \u001b[01;34m1\u001b[0m/\n",
            "drwxr-xr-x 2 root root 36864 Feb  1 22:06 \u001b[01;34m2\u001b[0m/\n",
            "drwxr-xr-x 2 root root 24576 Feb  1 22:06 \u001b[01;34m3\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# print(os.listdir('/content/dataset'))\n",
        "# os.rmdir('/content/dataset/.ipynb_checkpoints')\n",
        "print(os.listdir('/content/dataset'))\n",
        "from hugsvision.dataio.VisionDataset import VisionDataset\n",
        "import torch,torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "transform= transforms.Compose([transforms.Resize((223,223) ,interpolation=torchvision.transforms.InterpolationMode.BICUBIC)])\n",
        "train, test, id2label, label2id = VisionDataset.fromImageFolder(\n",
        "  \"/content/dataset\",\n",
        "  test_ratio   = 0.2,\n",
        "  balanced     =False,\n",
        "  augmentation = True,\n",
        "  transform =transform\n",
        "\n",
        "\n",
        ")\n",
        "print(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "QUpR3P_Z79gV",
        "outputId": "74ed29f3-af8b-4f17-9e19-c17e31ecca58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2', '3', '1']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1866aa838cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m223\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m223\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpolationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m train, test, id2label, label2id = VisionDataset.fromImageFolder(\n\u001b[1;32m     11\u001b[0m   \u001b[0;34m\"/content/dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'ReScale'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_model = 'facebook/deit-base-distilled-patch16-224' #facebook/deit-base-distilled-patch16-384' # 'google/vit-base-patch16-224-in21k'\n",
        "from hugsvision.nnet.VisionClassifierTrainer import VisionClassifierTrainer\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "\n",
        "trainer = VisionClassifierTrainer(\n",
        "\tmodel_name   = \"Vitmodel\",\n",
        "\ttrain        = train,\n",
        "\ttest         = test,\n",
        "\toutput_dir   = \"/content/drive/MyDrive/VIT_models\",\n",
        "\tmax_epochs   = 20,\n",
        "\tbatch_size   = 32, # On RTX 2080 Ti\n",
        "\tlr\t     = 2e-5,\n",
        "\tfp16\t     = True,\n",
        "\tmodel = ViTForImageClassification.from_pretrained(\n",
        "\t    huggingface_model,\n",
        "\t    num_labels = len(label2id),\n",
        "\t    label2id   = label2id,\n",
        "\t    id2label   = id2label\n",
        "\t),\n",
        "\tfeature_extractor = ViTFeatureExtractor.from_pretrained(\n",
        "\t\thuggingface_model,\n",
        "\t),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "icGtSFeA9Dsp",
        "outputId": "2dc5d234-6383-4bf0-b609-13cb37fd0fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing ViTForImageClassification: ['deit.encoder.layer.10.attention.attention.value.bias', 'deit.encoder.layer.2.attention.attention.query.weight', 'deit.encoder.layer.4.attention.attention.key.weight', 'deit.encoder.layer.11.attention.attention.value.weight', 'deit.encoder.layer.8.attention.attention.value.bias', 'deit.encoder.layer.6.attention.attention.query.weight', 'deit.encoder.layer.3.layernorm_after.bias', 'deit.encoder.layer.6.layernorm_after.bias', 'deit.encoder.layer.3.attention.output.dense.bias', 'deit.encoder.layer.9.output.dense.bias', 'deit.encoder.layer.11.attention.attention.query.bias', 'deit.encoder.layer.0.attention.output.dense.bias', 'deit.embeddings.position_embeddings', 'deit.encoder.layer.5.attention.attention.key.bias', 'deit.encoder.layer.1.attention.attention.value.weight', 'deit.encoder.layer.4.output.dense.bias', 'deit.encoder.layer.6.attention.attention.key.weight', 'deit.encoder.layer.1.layernorm_before.weight', 'deit.encoder.layer.3.attention.attention.key.bias', 'deit.encoder.layer.10.attention.attention.value.weight', 'deit.encoder.layer.9.attention.attention.key.weight', 'deit.encoder.layer.2.layernorm_after.weight', 'deit.encoder.layer.0.attention.output.dense.weight', 'deit.embeddings.patch_embeddings.projection.weight', 'deit.encoder.layer.7.attention.attention.query.bias', 'deit.encoder.layer.10.attention.output.dense.bias', 'deit.encoder.layer.9.attention.attention.key.bias', 'deit.encoder.layer.10.attention.output.dense.weight', 'deit.encoder.layer.11.layernorm_before.weight', 'deit.encoder.layer.5.output.dense.bias', 'deit.encoder.layer.2.layernorm_before.bias', 'deit.encoder.layer.11.output.dense.weight', 'deit.encoder.layer.11.intermediate.dense.weight', 'deit.encoder.layer.2.output.dense.weight', 'deit.encoder.layer.8.layernorm_after.bias', 'deit.encoder.layer.1.layernorm_after.bias', 'deit.encoder.layer.4.layernorm_before.weight', 'deit.encoder.layer.2.intermediate.dense.weight', 'deit.encoder.layer.10.layernorm_after.bias', 'deit.encoder.layer.8.attention.attention.key.bias', 'distillation_classifier.bias', 'deit.encoder.layer.8.attention.attention.query.bias', 'deit.encoder.layer.4.layernorm_before.bias', 'deit.encoder.layer.10.layernorm_before.bias', 'deit.encoder.layer.11.attention.attention.key.weight', 'deit.layernorm.bias', 'deit.encoder.layer.9.attention.attention.value.weight', 'deit.encoder.layer.3.attention.attention.query.bias', 'deit.encoder.layer.10.intermediate.dense.weight', 'deit.encoder.layer.4.attention.attention.value.bias', 'deit.encoder.layer.4.attention.output.dense.bias', 'deit.encoder.layer.11.attention.output.dense.weight', 'deit.encoder.layer.2.attention.output.dense.weight', 'deit.encoder.layer.7.attention.attention.value.bias', 'deit.encoder.layer.4.attention.output.dense.weight', 'deit.encoder.layer.7.intermediate.dense.bias', 'deit.encoder.layer.6.output.dense.weight', 'deit.encoder.layer.3.attention.attention.value.bias', 'deit.encoder.layer.1.intermediate.dense.weight', 'deit.encoder.layer.3.intermediate.dense.bias', 'deit.encoder.layer.4.attention.attention.value.weight', 'deit.encoder.layer.10.attention.attention.key.weight', 'deit.encoder.layer.11.attention.attention.query.weight', 'deit.encoder.layer.0.attention.attention.key.weight', 'deit.encoder.layer.1.attention.attention.key.weight', 'deit.encoder.layer.0.layernorm_before.weight', 'deit.encoder.layer.9.attention.attention.value.bias', 'deit.embeddings.cls_token', 'deit.encoder.layer.8.output.dense.weight', 'deit.encoder.layer.3.layernorm_before.bias', 'deit.encoder.layer.11.attention.output.dense.bias', 'deit.encoder.layer.5.layernorm_before.weight', 'deit.encoder.layer.4.layernorm_after.weight', 'deit.encoder.layer.10.intermediate.dense.bias', 'deit.encoder.layer.2.layernorm_after.bias', 'deit.encoder.layer.7.layernorm_after.weight', 'deit.encoder.layer.0.attention.attention.key.bias', 'deit.encoder.layer.5.intermediate.dense.bias', 'deit.encoder.layer.2.intermediate.dense.bias', 'deit.encoder.layer.5.attention.output.dense.weight', 'deit.encoder.layer.4.attention.attention.query.weight', 'deit.encoder.layer.7.attention.output.dense.bias', 'deit.encoder.layer.9.layernorm_before.weight', 'deit.encoder.layer.0.attention.attention.value.bias', 'deit.encoder.layer.0.attention.attention.value.weight', 'deit.encoder.layer.6.layernorm_before.weight', 'deit.encoder.layer.5.attention.attention.query.bias', 'deit.encoder.layer.0.layernorm_after.weight', 'deit.encoder.layer.1.attention.attention.query.weight', 'deit.encoder.layer.7.attention.attention.query.weight', 'deit.encoder.layer.9.intermediate.dense.weight', 'deit.encoder.layer.5.intermediate.dense.weight', 'deit.encoder.layer.8.layernorm_after.weight', 'deit.encoder.layer.11.layernorm_after.weight', 'deit.encoder.layer.0.attention.attention.query.bias', 'deit.encoder.layer.3.layernorm_before.weight', 'deit.encoder.layer.0.layernorm_before.bias', 'deit.encoder.layer.1.intermediate.dense.bias', 'deit.encoder.layer.0.layernorm_after.bias', 'deit.encoder.layer.9.output.dense.weight', 'deit.encoder.layer.7.attention.attention.key.weight', 'deit.encoder.layer.4.layernorm_after.bias', 'deit.encoder.layer.0.output.dense.bias', 'deit.encoder.layer.1.layernorm_before.bias', 'deit.encoder.layer.7.layernorm_before.bias', 'deit.encoder.layer.9.attention.output.dense.bias', 'deit.embeddings.distillation_token', 'deit.encoder.layer.4.intermediate.dense.weight', 'deit.encoder.layer.8.intermediate.dense.weight', 'deit.encoder.layer.3.attention.attention.value.weight', 'deit.encoder.layer.8.intermediate.dense.bias', 'deit.encoder.layer.10.attention.attention.query.bias', 'deit.encoder.layer.5.layernorm_after.bias', 'deit.encoder.layer.10.output.dense.weight', 'deit.encoder.layer.6.attention.attention.key.bias', 'deit.encoder.layer.7.attention.attention.key.bias', 'deit.encoder.layer.6.output.dense.bias', 'deit.encoder.layer.7.intermediate.dense.weight', 'deit.encoder.layer.11.output.dense.bias', 'deit.layernorm.weight', 'deit.encoder.layer.6.attention.output.dense.bias', 'deit.encoder.layer.3.layernorm_after.weight', 'deit.encoder.layer.6.layernorm_after.weight', 'cls_classifier.bias', 'deit.encoder.layer.2.attention.output.dense.bias', 'deit.encoder.layer.9.attention.attention.query.weight', 'deit.encoder.layer.9.intermediate.dense.bias', 'distillation_classifier.weight', 'deit.encoder.layer.4.attention.attention.query.bias', 'deit.encoder.layer.7.layernorm_before.weight', 'deit.encoder.layer.8.attention.attention.key.weight', 'deit.encoder.layer.10.layernorm_before.weight', 'deit.encoder.layer.1.attention.output.dense.bias', 'deit.encoder.layer.11.attention.attention.key.bias', 'deit.encoder.layer.6.intermediate.dense.bias', 'deit.encoder.layer.7.attention.output.dense.weight', 'deit.encoder.layer.8.attention.attention.query.weight', 'deit.encoder.layer.7.attention.attention.value.weight', 'deit.encoder.layer.0.intermediate.dense.bias', 'deit.encoder.layer.9.layernorm_after.bias', 'deit.encoder.layer.3.output.dense.weight', 'deit.encoder.layer.2.attention.attention.query.bias', 'deit.encoder.layer.8.output.dense.bias', 'deit.encoder.layer.11.layernorm_after.bias', 'deit.encoder.layer.11.layernorm_before.bias', 'deit.encoder.layer.11.attention.attention.value.bias', 'deit.embeddings.patch_embeddings.projection.bias', 'deit.encoder.layer.4.attention.attention.key.bias', 'deit.encoder.layer.5.layernorm_after.weight', 'deit.encoder.layer.4.output.dense.weight', 'deit.encoder.layer.6.intermediate.dense.weight', 'deit.encoder.layer.4.intermediate.dense.bias', 'deit.encoder.layer.7.output.dense.weight', 'deit.encoder.layer.5.attention.output.dense.bias', 'deit.encoder.layer.8.attention.attention.value.weight', 'deit.encoder.layer.9.layernorm_before.bias', 'deit.encoder.layer.9.layernorm_after.weight', 'deit.encoder.layer.5.attention.attention.query.weight', 'deit.encoder.layer.6.layernorm_before.bias', 'deit.encoder.layer.0.output.dense.weight', 'deit.encoder.layer.11.intermediate.dense.bias', 'deit.encoder.layer.2.attention.attention.key.bias', 'deit.encoder.layer.7.output.dense.bias', 'deit.encoder.layer.10.attention.attention.query.weight', 'deit.encoder.layer.1.attention.attention.query.bias', 'deit.encoder.layer.10.attention.attention.key.bias', 'deit.encoder.layer.3.output.dense.bias', 'deit.encoder.layer.5.attention.attention.key.weight', 'deit.encoder.layer.2.output.dense.bias', 'deit.encoder.layer.5.attention.attention.value.weight', 'deit.encoder.layer.1.attention.attention.key.bias', 'deit.encoder.layer.3.attention.attention.key.weight', 'deit.encoder.layer.10.layernorm_after.weight', 'deit.encoder.layer.2.attention.attention.value.bias', 'deit.encoder.layer.9.attention.output.dense.weight', 'deit.encoder.layer.9.attention.attention.query.bias', 'deit.encoder.layer.5.layernorm_before.bias', 'deit.encoder.layer.3.attention.attention.query.weight', 'deit.encoder.layer.3.intermediate.dense.weight', 'deit.encoder.layer.0.intermediate.dense.weight', 'deit.encoder.layer.6.attention.attention.query.bias', 'deit.encoder.layer.1.output.dense.bias', 'deit.encoder.layer.6.attention.attention.value.bias', 'deit.encoder.layer.6.attention.output.dense.weight', 'deit.encoder.layer.7.layernorm_after.bias', 'deit.encoder.layer.8.attention.output.dense.weight', 'deit.encoder.layer.8.attention.output.dense.bias', 'deit.encoder.layer.3.attention.output.dense.weight', 'deit.encoder.layer.8.layernorm_before.bias', 'deit.encoder.layer.1.attention.output.dense.weight', 'deit.encoder.layer.1.layernorm_after.weight', 'deit.encoder.layer.5.output.dense.weight', 'deit.encoder.layer.6.attention.attention.value.weight', 'deit.encoder.layer.1.attention.attention.value.bias', 'deit.encoder.layer.2.attention.attention.key.weight', 'deit.encoder.layer.2.layernorm_before.weight', 'deit.encoder.layer.10.output.dense.bias', 'deit.encoder.layer.0.attention.attention.query.weight', 'deit.encoder.layer.2.attention.attention.value.weight', 'cls_classifier.weight', 'deit.encoder.layer.1.output.dense.weight', 'deit.encoder.layer.5.attention.attention.value.bias', 'deit.encoder.layer.8.layernorm_before.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.10.attention.attention.value.weight', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.attention.attention.key.weight', 'layernorm.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'layernorm.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.attention.attention.query.bias', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.6.output.dense.weight', 'classifier.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.1.attention.attention.value.weight', 'embeddings.cls_token', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.0.attention.attention.key.bias', 'embeddings.position_embeddings', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': '1', '1': '2', '2': '3'}\n",
            "{'1': '0', '2': '1', '3': '2'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 2451\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1540\n",
            "  Number of trainable parameters = 85800963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer builded!\n",
            "Start Training!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2d7dbcb3b77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViTFeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mViTForImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m trainer = VisionClassifierTrainer(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel_name\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m\"Vitmodel\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/hugsvision/nnet/VisionClassifierTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, model_name, model, feature_extractor, train, test, max_epochs, cores, batch_size, lr, eval_metric, fp16, classification_report_digits, checkpoint_path)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Your checkpoint is loaded from: {self.checkpoint_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/trainer/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         )\n\u001b[0;32m-> 1543\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1789\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2571\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2572\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         outputs = self.vit(\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    115\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m    116\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbool_masked_pos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0;34mf\"Input image size ({height}*{width}) doesn't match model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0;34mf\" ({self.image_size[0]}*{self.image_size[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input image size (256*256) doesn't match model (224*224)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ref, hyp = trainer.evaluate_f1_score()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mt1-kiGAXOh",
        "outputId": "dfa19f2d-3e5c-4a60-d840-3582f8d3557a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 425/425 [00:15<00:00, 27.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.9533    0.9346    0.9439       153\n",
            "           2     0.9400    0.9592    0.9495       147\n",
            "           3     0.9760    0.9760    0.9760       125\n",
            "\n",
            "    accuracy                         0.9553       425\n",
            "   macro avg     0.9564    0.9566    0.9565       425\n",
            "weighted avg     0.9554    0.9553    0.9553       425\n",
            "\n",
            "Logs saved at: \u001b[93m/content/drive/MyDrive/VIT_models/VITMODEL/20_2023-02-01-22-24-43\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "acc = accuracy_score(ref,hyp) *100\n",
        "print(acc)\n",
        "\n",
        "cm = confusion_matrix(ref, hyp)\n",
        "labels = list(label2id.keys())\n",
        "df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
        "\n",
        "plt.figure(figsize = (5,5))\n",
        "plt.title(\"Confusion Matraix with Accuracy {} %\".format(str(acc)[0:5]))\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 8}, fmt=\"\",cmap= plt.cm.CMRmap)\n",
        "plt.savefig(\"/content/drive/MyDrive/VIT_models/conf_matrix_3.jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_ZQc_-wBpmL",
        "outputId": "6d0c1187-c8fe-4b83-8207-678f813f0128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95.52941176470588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "conf = cv2.imread(\"/content/drive/MyDrive/VIT_models/conf_matrix_3.jpg\")\n",
        "plt.figure()\n",
        "cv2_imshow(conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "Gm0D62qgBxp_",
        "outputId": "2527fbda-10a1-42e2-c9e5-68f831fa7285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=360x360 at 0x7F3B000BAF40>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEBv6KmFB179"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}